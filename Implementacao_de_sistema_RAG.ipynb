{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vo3cgJxY0CCP",
        "zsAs_vZP0aLW",
        "o6fOYnTr0fS1",
        "2OEHbqUW1_AG",
        "u-R8_-Vr4dZ6",
        "MGgJYNn-4v2A",
        "kRzohJvD424A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contextualização e resumo do notebook"
      ],
      "metadata": {
        "id": "5wvrwiIWIpCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Arquitetura implementada\n",
        "\n",
        "Neste trabalho, foi desenvolvido e implementado um sistema completo de Retrieval-Augmented Generation (RAG), integrando componentes de recuperação e geração de informações de forma coesa e eficiente.\n",
        "\n",
        "Para recuperação, empregou-se um pipeline de busca vetorial, utilizando o modelo all-MiniLM-L6-v2, disponibilizado pela biblioteca Hugging Face, para a geração de embeddings (representações vetoriais) a partir dos documentos textuais. Esses vetores foram, então, armazenados e indexados em um banco de dados vetorial em memória, por meio da ferramenta FAISS (Facebook AI Similarity Search), possibilitando a recuperação semântica eficiente das informações mais relevantes.\n",
        "\n",
        "Para geração de respostas, adotou-se um pipeline extrativo de Question Answering (QA) baseado no modelo distilbert-base-cased-distilled-squad, também proveniente do Hugging Face, o qual foi previamente treinado em um conjunto de dados apropriado para a tarefa de extração de respostas precisas a partir dos textos recuperados. Essa arquitetura integrada permitiu o funcionamento harmonioso do sistema RAG, combinando técnicas de recuperação e geração para aprimorar a qualidade e a relevância das respostas produzidas.\n",
        "\n",
        "## 2. Desafios encontrados\n",
        "\n",
        "Durante o desenvolvimento do sistema, foi necessário lidar com questões relacionadas às dependências e à compatibilidade entre bibliotecas. A biblioteca LangChain passou por atualizações recentes que modificaram os locais de instalação dos modelos de embedding, exigindo ajustes nos caminhos de importação e configuração para garantir o correto funcionamento do ambiente. Além disso, foram identificados conflitos entre as bibliotecas LangChain e Hugging Face, especificamente na geração de embeddings, que precisaram ser resolvidos para assegurar a integração estável e eficiente dos componentes do sistema.\n",
        "\n",
        "No que se refere à qualidade da geração de respostas, o modelo distilbert-base-cased-distilled-squad, utilizado neste projeto, é de natureza extrativa, e não generativa. Isso implica que o modelo é capaz apenas de reproduzir trechos exatos do contexto fornecido, sem criar novas formulações ou inferências. Como resultado, as respostas produzidas tendem a ser mais simples e, em alguns casos, imprecisas, o que limita a fluidez e a abrangência das informações geradas.\n",
        "\n",
        "## 3. Resultados obtidos\n",
        "\n",
        "Durante os testes, observou-se que o módulo de Question Answering, baseado no modelo distilbert-base-cased-distilled-squad, foi capaz de identificar trechos pertinentes aos contextos das perguntas, fornecendo respostas diretas e concisas, embora incorretas em alguns cenários. Além disso, no módulo de avaliação do sistema, ficou claro que as respostas fornecidas pelo modelo são demasiadamente simples, o que pode ou não ser ideal em alguns casos. Isso se deve ao fato de que, como se trata de um modelo extrativo, as respostas limitaram-se a reproduzir fragmentos do texto original, sem gerar reformulações ou inferências adicionais. De modo geral, o sistema mostrou-se funcional e coerente com a proposta de combinar busca semântica e extração automática de informações, validando a arquitetura implementada.\n",
        "\n",
        "## 4. Possíveis melhorias\n",
        "\n",
        "Embora o sistema RAG desenvolvido tenha alcançado seus objetivos principais, existem diversas oportunidades de aprimoramento. Uma primeira melhoria, a depender do caso de uso, seria a substituição do modelo extrativo por um modelo generativo, como o Flan-T5 ou o Llama 3, que permitiria respostas mais completas e contextualizadas, superando as limitações de extração literal do DistilBERT. Além disso, seria benéfico expandir o volume de dados utilizados, incluindo uma porção maior do dataset SQuAD ou outros conjuntos de perguntas e respostas, a fim de aumentar a robustez e a generalização do sistema.\n",
        "\n",
        "Do ponto de vista estrutural, a utilização de um banco vetorial persistente (como ChromaDB ou Pinecone) poderia substituir o armazenamento em memória do FAISS, garantindo escalabilidade e reuso do índice em diferentes sessões. Por fim, a otimização de desempenho por meio de batching e aceleração em GPU, bem como a criação de uma interface interativa para consulta do sistema, representariam avanços significativos tanto em eficiência quanto em usabilidade."
      ],
      "metadata": {
        "id": "TtYdEHFDIvRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação e importação de dependências"
      ],
      "metadata": {
        "id": "Vo3cgJxY0CCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Framework RAG\n",
        "!pip install langchain langchain-community langchain-huggingface\n",
        "\n",
        "# Embeddings e Modelos (Hugging Face)\n",
        "!pip install sentence-transformers transformers torch accelerate datasets\n",
        "\n",
        "# Banco de dados vetorial\n",
        "!pip install chromadb faiss-cpu"
      ],
      "metadata": {
        "id": "3DQ1jIGwjb9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Tj8vzd9VkAM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura do dataset utilizado como RAG"
      ],
      "metadata": {
        "id": "krDJUT_T0MAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualização do dataset\n",
        "\n",
        "O dataset escolhido foi o **SQuAD v2** (Stanford Question Answering Dataset 2.0), que é uma versão aprimorada do famoso SQuAD 1.1, amplamente usado para treinamento e avaliação de modelos de compreensão de linguagem natural (NLP), especialmente em tarefas de Pergunta e Resposta (Question Answering).\n",
        "\n",
        "**Formato dos dados:** cada amostra contém:\n",
        "\n",
        "* **context:** o texto base (geralmente um parágrafo da Wikipédia);\n",
        "\n",
        "* **question:** a pergunta feita sobre o contexto;\n",
        "\n",
        "* **answers:** contém as respostas corretas (text e answer_start);\n",
        "\n",
        "* **id:** identificador único da amostra."
      ],
      "metadata": {
        "id": "zsAs_vZP0aLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"squad_v2\", split=\"train[:100]\") # Primeiras 100 entradas\n",
        "\n",
        "# Transformando em dataframe do pandas\n",
        "df = dataset.to_pandas()\n",
        "print(df.shape)\n",
        "df"
      ],
      "metadata": {
        "id": "pMOcRPPYpSbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação dos dados"
      ],
      "metadata": {
        "id": "o6fOYnTr0fS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sample_data():\n",
        "  \"\"\"Carrega um dataset de QA para demonstração\"\"\"\n",
        "  dataset = load_dataset(\"squad_v2\", split=\"train[:100]\") # Primeiras 100 entradas\n",
        "  documents = []\n",
        "\n",
        "  for item in dataset:\n",
        "    # Criar documentos a partir do contexto\n",
        "    doc = Document(\n",
        "      page_content=item[\"context\"],\n",
        "      metadata={\n",
        "        \"title\": f\"Artigo_{item['id']}\",\n",
        "        \"question\": item[\"question\"],\n",
        "        \"answers\": item[\"answers\"]\n",
        "      }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "Pq-AdN5tkCB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os dados\n",
        "documents = load_sample_data()\n",
        "print(f\"Carregados {len(documents)} documentos\")\n",
        "\n",
        "for i, doc in enumerate(documents[:5]):\n",
        "  print(f\"Documento {i+1}: {doc.page_content[:100]}...\")"
      ],
      "metadata": {
        "id": "RP9lCnTLkf2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepearação do banco de dados vetorial"
      ],
      "metadata": {
        "id": "2OEHbqUW1_AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def setup_retrieval_system(documents):\n",
        "    \"\"\"Configura o sistema de recuperação vetorial.\n",
        "    Transforma documentos de texto em vetores numéricos (embeddings) e os armazena em um banco vetorial (FAISS), permitindo buscas semânticas eficientes.\"\"\"\n",
        "\n",
        "    # Divide documentos em chunks menores (até 300 caracteres com 50 caracteres de sobreposição entre os trechos consecutivos)\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=50,\n",
        "        separator=\"\\n\"\n",
        "    )\n",
        "\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Documentos divididos em {len(texts)} chunks\")\n",
        "\n",
        "    # Carrega modelo de embeddings do Hugging Face (compatível com LangChain)\n",
        "    # all-MiniLM-L6_v2: modelo pequeno e rápido do Hugging Face para similaridade semântica e tarefas de recuperação\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "    # Cria índice de busca vetorial com FAISS\n",
        "    vector_store = FAISS.from_documents(texts, embedding_model)\n",
        "\n",
        "    return vector_store, embedding_model, texts\n",
        "\n",
        "# Inicializa o sistema de recuperação\n",
        "vector_store, embedding_model, text_chunks = setup_retrieval_system(documents)"
      ],
      "metadata": {
        "id": "dOvqgfY5kwrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo sistema RAG"
      ],
      "metadata": {
        "id": "u-R8_-Vr4dZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o modelo de QA do Hugging Face\n",
        "def setup_generator():\n",
        "  \"\"\"Configura o pipeline de Question Answering\"\"\"\n",
        "\n",
        "  # Definindo um pipeline pré-pronto\n",
        "  # distilbert-base-cased-distilled-squad: modelo treinado especificamente no dataset escolhido\n",
        "  qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"distilbert-base-cased-distilled-squad\",\n",
        "    tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
        "  )\n",
        "\n",
        "  return qa_pipeline\n",
        "\n",
        "# Inicializando o modelo de QA\n",
        "print(\"Carregando modelo de Question Answering...\")\n",
        "qa_pipeline = setup_generator()\n",
        "print(\"Modelo carregado com sucesso!\")"
      ],
      "metadata": {
        "id": "iAUs1ifIk-rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementação o sistema RAG completo\n",
        "class SimpleRAGSystem:\n",
        "  def __init__(self, vector_store, qa_pipeline, top_k=4):\n",
        "    self.vector_store = vector_store\n",
        "    self.qa_pipeline = qa_pipeline\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def retrieve_documents(self, question):\n",
        "    \"\"\"Recupera os documentos mais relevantes\"\"\"\n",
        "    # Buscar documentos similares\n",
        "    docs = self.vector_store.similarity_search(question, k=self.top_k)\n",
        "    return docs\n",
        "\n",
        "  def create_context(self, documents):\n",
        "    \"\"\"Combina os documentos em um único contexto\"\"\"\n",
        "    contexts = [doc.page_content for doc in documents]\n",
        "    return \"\\n\\n\".join(contexts)\n",
        "\n",
        "  def generate_answer(self, question, context):\n",
        "    \"\"\"Gera resposta usando o pipeline de QA\"\"\"\n",
        "    try:\n",
        "      # Formata entrada para o pipeline\n",
        "      qa_input = {\n",
        "      'question': question,\n",
        "      'context': context\n",
        "      }\n",
        "\n",
        "      # Gera resposta\n",
        "      result = self.qa_pipeline(qa_input)\n",
        "      return result\n",
        "    except Exception as e:\n",
        "      return {\"answer\": f\"Erro ao gerar resposta: {str(e)}\", \"score\": 0.0}\n",
        "\n",
        "  def query(self, question):\n",
        "    \"\"\"Método principal para fazer consultas ao sistema RAG\"\"\"\n",
        "    print(f\"Pergunta: {question}\")\n",
        "\n",
        "    # Fase de recuperação\n",
        "    relevant_docs = self.retrieve_documents(question)\n",
        "    print(f\"Documentos recuperados: {len(relevant_docs)}\")\n",
        "\n",
        "    # Fase de criação de contexto\n",
        "    context = self.create_context(relevant_docs)\n",
        "\n",
        "    # Fase de geração da resposta\n",
        "    answer_result = self.generate_answer(question, context)\n",
        "    return {\n",
        "    \"question\": question,\n",
        "    \"answer\": answer_result[\"answer\"],\n",
        "    \"confidence\": answer_result[\"score\"],\n",
        "    \"source_documents\": relevant_docs,\n",
        "    \"context_used\": context\n",
        "    }\n",
        "\n",
        "# Inicializando o sistema RAG\n",
        "rag_system = SimpleRAGSystem(vector_store, qa_pipeline, top_k=5) # Defina top_k como o número de documentos utilizados como contexto para a resposta"
      ],
      "metadata": {
        "id": "MwvHhmrplMQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes"
      ],
      "metadata": {
        "id": "MGgJYNn-4v2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o sistema com exemplos práticos\n",
        "def test_rag_system():\n",
        "  \"\"\"Testa o sistema RAG com várias perguntas\"\"\"\n",
        "\n",
        "  test_questions = [\n",
        "  \"How many years did Beyoncé's career last?\",\n",
        "  \"How much records did Beyoncé sold as a solo artist?\",\n",
        "  \"What are the themes of Beyoncé songs?\",\n",
        "  \"Where was Beyoncé born?\",\n",
        "  \"Who is Beyoncé?\"\n",
        "  ]\n",
        "\n",
        "  for question in test_questions:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    result = rag_system.query(question)\n",
        "\n",
        "    print(f\"Resposta: {result['answer']}\")\n",
        "    print(f\"Confiança: {result['confidence']:.4f}\")\n",
        "    print(f\"Fontes usadas:\")\n",
        "\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "      print(f\" {i+1}. {doc.page_content}...\")\n",
        "      print(\"-\"*60)\n",
        "\n",
        "# Executar testes\n",
        "test_rag_system()"
      ],
      "metadata": {
        "id": "NK7lsc7Xl3ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação do sistema"
      ],
      "metadata": {
        "id": "kRzohJvD424A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do sistema\n",
        "def evaluate_rag_system():\n",
        "  \"\"\"Avalia o desempenho do sistema RAG\"\"\"\n",
        "\n",
        "  # Perguntas de teste com respostas esperadas\n",
        "  test_cases = [\n",
        "  {\n",
        "  \"question\": \"How many years did Beyoncé's career last?\",\n",
        "  \"expected_keywords\": [\"years\", \"span\", \"Beyoncé\", \"19\"]\n",
        "  },\n",
        "  {\n",
        "  \"question\": \"What are the themes of Beyoncé songs?\",\n",
        "  \"expected_keywords\": [\"themes\", \"songs\", \"Beyoncé\", \"darker\"]\n",
        "  }\n",
        "  ]\n",
        "\n",
        "  print(\"Avaliando Sistema RAG\")\n",
        "  print(\"-\" * 40)\n",
        "\n",
        "  for i, test_case in enumerate(test_cases):\n",
        "    result = rag_system.query(test_case[\"question\"])\n",
        "\n",
        "    print(f\"\\nTeste {i+1}:\")\n",
        "    print(f\"Pergunta: {test_case['question']}\")\n",
        "    print(f\"Resposta: {result['answer']}\")\n",
        "    print(f\"Confiança: {result['confidence']:.4f}\")\n",
        "\n",
        "    # Verificar se contém palavras-chave esperadas\n",
        "    answer_lower = result['answer'].lower()\n",
        "    keywords_found = [kw for kw in test_case[\"expected_keywords\"] if kw\n",
        "    in answer_lower]\n",
        "\n",
        "    print(f\"Palavras-chave encontradas: {keywords_found}\")\n",
        "    print(f\"Score:{len(keywords_found)}/{len(test_case['expected_keywords'])}\\n\")\n",
        "\n",
        "# Executar avaliação\n",
        "evaluate_rag_system()"
      ],
      "metadata": {
        "id": "MpByX1Psmbhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}