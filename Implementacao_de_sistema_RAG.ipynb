{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vo3cgJxY0CCP",
        "zsAs_vZP0aLW",
        "o6fOYnTr0fS1",
        "2OEHbqUW1_AG",
        "u-R8_-Vr4dZ6",
        "MGgJYNn-4v2A",
        "kRzohJvD424A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação e importação de dependências"
      ],
      "metadata": {
        "id": "Vo3cgJxY0CCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Framework RAG\n",
        "!pip install langchain langchain-community langchain-huggingface\n",
        "\n",
        "# Embeddings e Modelos (Hugging Face)\n",
        "!pip install sentence-transformers transformers torch accelerate datasets\n",
        "\n",
        "# Banco de dados vetorial\n",
        "!pip install chromadb faiss-cpu"
      ],
      "metadata": {
        "id": "3DQ1jIGwjb9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Tj8vzd9VkAM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura do dataset utilizado como RAG"
      ],
      "metadata": {
        "id": "krDJUT_T0MAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualização do dataset\n",
        "\n",
        "O dataset escolhido foi o **SQuAD v2** (Stanford Question Answering Dataset 2.0), que é uma versão aprimorada do famoso SQuAD 1.1, amplamente usado para treinamento e avaliação de modelos de compreensão de linguagem natural (NLP), especialmente em tarefas de Pergunta e Resposta (Question Answering).\n",
        "\n",
        "**Formato dos dados:** cada amostra contém:\n",
        "\n",
        "* **context:** o texto base (geralmente um parágrafo da Wikipédia);\n",
        "\n",
        "* **question:** a pergunta feita sobre o contexto;\n",
        "\n",
        "* **answers:** contém as respostas corretas (text e answer_start);\n",
        "\n",
        "* **id:** identificador único da amostra."
      ],
      "metadata": {
        "id": "zsAs_vZP0aLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"squad_v2\", split=\"train[:100]\") # Primeiras 100 entradas\n",
        "\n",
        "# Transformando em dataframe do pandas\n",
        "df = dataset.to_pandas()\n",
        "print(df.shape)\n",
        "df"
      ],
      "metadata": {
        "id": "pMOcRPPYpSbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação dos dados"
      ],
      "metadata": {
        "id": "o6fOYnTr0fS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sample_data():\n",
        "  \"\"\"Carrega um dataset de QA para demonstração\"\"\"\n",
        "  dataset = load_dataset(\"squad_v2\", split=\"train[:100]\") # Primeiras 100 entradas\n",
        "  documents = []\n",
        "\n",
        "  for item in dataset:\n",
        "    # Criar documentos a partir do contexto\n",
        "    doc = Document(\n",
        "      page_content=item[\"context\"],\n",
        "      metadata={\n",
        "        \"title\": f\"Artigo_{item['id']}\",\n",
        "        \"question\": item[\"question\"],\n",
        "        \"answers\": item[\"answers\"]\n",
        "      }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "Pq-AdN5tkCB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os dados\n",
        "documents = load_sample_data()\n",
        "print(f\"Carregados {len(documents)} documentos\")\n",
        "\n",
        "for i, doc in enumerate(documents[:5]):\n",
        "  print(f\"Documento {i+1}: {doc.page_content[:100]}...\")"
      ],
      "metadata": {
        "id": "RP9lCnTLkf2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepearação do banco de dados vetorial"
      ],
      "metadata": {
        "id": "2OEHbqUW1_AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def setup_retrieval_system(documents):\n",
        "    \"\"\"Configura o sistema de recuperação vetorial.\n",
        "    Transforma documentos de texto em vetores numéricos (embeddings) e os armazena em um banco vetorial (FAISS), permitindo buscas semânticas eficientes.\"\"\"\n",
        "\n",
        "    # Divide documentos em chunks menores (até 300 caracteres com 50 caracteres de sobreposição entre os trechos consecutivos)\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=50,\n",
        "        separator=\"\\n\"\n",
        "    )\n",
        "\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Documentos divididos em {len(texts)} chunks\")\n",
        "\n",
        "    # Carrega modelo de embeddings do Hugging Face (compatível com LangChain)\n",
        "    # all-MiniLM-L6_v2: modelo pequeno e rápido do Hugging Face para similaridade semântica e tarefas de recuperação\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "    # Cria índice de busca vetorial com FAISS\n",
        "    vector_store = FAISS.from_documents(texts, embedding_model)\n",
        "\n",
        "    return vector_store, embedding_model, texts\n",
        "\n",
        "# Inicializa o sistema de recuperação\n",
        "vector_store, embedding_model, text_chunks = setup_retrieval_system(documents)"
      ],
      "metadata": {
        "id": "dOvqgfY5kwrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo sistema RAG"
      ],
      "metadata": {
        "id": "u-R8_-Vr4dZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o modelo de QA do Hugging Face\n",
        "def setup_generator():\n",
        "  \"\"\"Configura o pipeline de Question Answering\"\"\"\n",
        "\n",
        "  # Definindo um pipeline pré-pronto\n",
        "  # distilbert-base-cased-distilled-squad: modelo treinado especificamente no dataset escolhido\n",
        "  qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"distilbert-base-cased-distilled-squad\",\n",
        "    tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
        "  )\n",
        "\n",
        "  return qa_pipeline\n",
        "\n",
        "# Inicializando o modelo de QA\n",
        "print(\"Carregando modelo de Question Answering...\")\n",
        "qa_pipeline = setup_generator()\n",
        "print(\"Modelo carregado com sucesso!\")"
      ],
      "metadata": {
        "id": "iAUs1ifIk-rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementação o sistema RAG completo\n",
        "class SimpleRAGSystem:\n",
        "  def __init__(self, vector_store, qa_pipeline, top_k=4):\n",
        "    self.vector_store = vector_store\n",
        "    self.qa_pipeline = qa_pipeline\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def retrieve_documents(self, question):\n",
        "    \"\"\"Recupera os documentos mais relevantes\"\"\"\n",
        "    # Buscar documentos similares\n",
        "    docs = self.vector_store.similarity_search(question, k=self.top_k)\n",
        "    return docs\n",
        "\n",
        "  def create_context(self, documents):\n",
        "    \"\"\"Combina os documentos em um único contexto\"\"\"\n",
        "    contexts = [doc.page_content for doc in documents]\n",
        "    return \"\\n\\n\".join(contexts)\n",
        "\n",
        "  def generate_answer(self, question, context):\n",
        "    \"\"\"Gera resposta usando o pipeline de QA\"\"\"\n",
        "    try:\n",
        "      # Formata entrada para o pipeline\n",
        "      qa_input = {\n",
        "      'question': question,\n",
        "      'context': context\n",
        "      }\n",
        "\n",
        "      # Gera resposta\n",
        "      result = self.qa_pipeline(qa_input)\n",
        "      return result\n",
        "    except Exception as e:\n",
        "      return {\"answer\": f\"Erro ao gerar resposta: {str(e)}\", \"score\": 0.0}\n",
        "\n",
        "  def query(self, question):\n",
        "    \"\"\"Método principal para fazer consultas ao sistema RAG\"\"\"\n",
        "    print(f\"Pergunta: {question}\")\n",
        "\n",
        "    # Fase de recuperação\n",
        "    relevant_docs = self.retrieve_documents(question)\n",
        "    print(f\"Documentos recuperados: {len(relevant_docs)}\")\n",
        "\n",
        "    # Fase de criação de contexto\n",
        "    context = self.create_context(relevant_docs)\n",
        "\n",
        "    # Fase de geração da resposta\n",
        "    answer_result = self.generate_answer(question, context)\n",
        "    return {\n",
        "    \"question\": question,\n",
        "    \"answer\": answer_result[\"answer\"],\n",
        "    \"confidence\": answer_result[\"score\"],\n",
        "    \"source_documents\": relevant_docs,\n",
        "    \"context_used\": context\n",
        "    }\n",
        "\n",
        "# Inicializando o sistema RAG\n",
        "rag_system = SimpleRAGSystem(vector_store, qa_pipeline, top_k=5) # Defina top_k como o número de documentos utilizados como contexto para a resposta"
      ],
      "metadata": {
        "id": "MwvHhmrplMQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes"
      ],
      "metadata": {
        "id": "MGgJYNn-4v2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o sistema com exemplos práticos\n",
        "def test_rag_system():\n",
        "  \"\"\"Testa o sistema RAG com várias perguntas\"\"\"\n",
        "\n",
        "  test_questions = [\n",
        "  \"How many years did Beyoncé's career last?\",\n",
        "  \"How much records did Beyoncé sold as a solo artist?\",\n",
        "  \"What are the themes of Beyoncé songs?\",\n",
        "  \"Where was Beyoncé born?\",\n",
        "  \"Who is Beyoncé?\"\n",
        "  ]\n",
        "\n",
        "  for question in test_questions:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    result = rag_system.query(question)\n",
        "\n",
        "    print(f\"Resposta: {result['answer']}\")\n",
        "    print(f\"Confiança: {result['confidence']:.4f}\")\n",
        "    print(f\"Fontes usadas:\")\n",
        "\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "      print(f\" {i+1}. {doc.page_content}...\")\n",
        "      print(\"-\"*60)\n",
        "\n",
        "# Executar testes\n",
        "test_rag_system()"
      ],
      "metadata": {
        "id": "NK7lsc7Xl3ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação do sistema"
      ],
      "metadata": {
        "id": "kRzohJvD424A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do sistema\n",
        "def evaluate_rag_system():\n",
        "  \"\"\"Avalia o desempenho do sistema RAG\"\"\"\n",
        "\n",
        "  # Perguntas de teste com respostas esperadas\n",
        "  test_cases = [\n",
        "  {\n",
        "  \"question\": \"How many years did Beyoncé's career last?\",\n",
        "  \"expected_keywords\": [\"years\", \"span\", \"Beyoncé\", \"19\"]\n",
        "  },\n",
        "  {\n",
        "  \"question\": \"What are the themes of Beyoncé songs?\",\n",
        "  \"expected_keywords\": [\"themes\", \"songs\", \"Beyoncé\", \"darker\"]\n",
        "  }\n",
        "  ]\n",
        "\n",
        "  print(\"Avaliando Sistema RAG\")\n",
        "  print(\"-\" * 40)\n",
        "\n",
        "  for i, test_case in enumerate(test_cases):\n",
        "    result = rag_system.query(test_case[\"question\"])\n",
        "\n",
        "    print(f\"\\nTeste {i+1}:\")\n",
        "    print(f\"Pergunta: {test_case['question']}\")\n",
        "    print(f\"Resposta: {result['answer']}\")\n",
        "    print(f\"Confiança: {result['confidence']:.4f}\")\n",
        "\n",
        "    # Verificar se contém palavras-chave esperadas\n",
        "    answer_lower = result['answer'].lower()\n",
        "    keywords_found = [kw for kw in test_case[\"expected_keywords\"] if kw\n",
        "    in answer_lower]\n",
        "\n",
        "    print(f\"Palavras-chave encontradas: {keywords_found}\")\n",
        "    print(f\"Score:{len(keywords_found)}/{len(test_case['expected_keywords'])}\\n\")\n",
        "\n",
        "# Executar avaliação\n",
        "evaluate_rag_system()"
      ],
      "metadata": {
        "id": "MpByX1Psmbhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão"
      ],
      "metadata": {
        "id": "uyslur4c8Bmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo é capaz de utilizar a base vetorial como contexto, caracterizando-se como um modelo RAG funcional. No entanto, as respostas nem sempre são corretas e claras, embora o modelo utilizado tenha sido treinado com a base de dados selecionada. Melhorias na base e no modelo, bem como a utilização de engenharia de prompts poderiam melhorar o desempenho do modelo, o que será feito em um trabalho futuro."
      ],
      "metadata": {
        "id": "cm19HM138DWt"
      }
    }
  ]
}